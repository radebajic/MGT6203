library(dplyr)
library(lubridate)
library(tidyverse)

### Load datasets
file_path <- "C:/Users/bzg/OneDrive - EY/Desktop/edx MM/3. Data Analytics for Business/Group Project/data/commodity-prices-2016.csv"
data_commodity <- read.csv(file_path)
head(data_commodity)

file_path <- "C:/Users/bzg/OneDrive - EY/Desktop/edx MM/3. Data Analytics for Business/Group Project/data/average_monthly_temperature_by_state_1950-2022.csv"
data_weather <- read.csv(file_path)
head(data_weather)

file_path <- "C:/Users/bzg/OneDrive - EY/Desktop/edx MM/3. Data Analytics for Business/Group Project/data/US_macroeconomics.csv"
data_macro <- read.csv(file_path)
head(data_macro)

### Change to date type
data_commodity$Date <- as.Date(data_commodity$Date)
data_macro$Date <- as.Date(data_macro$date)
data_weather$Date <- as.Date(paste(data_weather$year, data_weather$month, "01", sep='-'))

### Clean Commodity dataset
na_columns <- colSums(is.na(data_commodity)) >0 # there are 5-6 columns with NA values. As these seem not to be very important, we delete these columns
# print(names(na_columns)[na_columns])
data_commodity <- data_commodity[, !na_columns]
data_commodity_filt <- data_commodity %>%
  filter(Date >= as.Date('1980-11-01') & Date <= as.Date('2016-02-01')) 

### Clean Weather dataset
na_columns <- colSums(is.na(data_weather)) >0 # there are no NA values in the dataset
#print(names(na_columns)[na_columns])
data_weather <- data_weather[c('Date', 'state', 'average_temp')] #only keep relevant columns for analysis
data_weather <- pivot_wider(data_weather, names_from = state, values_from = average_temp) # pivot states in columns
data_weather_filt <- data_weather %>%
  filter(Date >= as.Date('1980-11-01') & Date <= as.Date('2016-02-01')) 

### Clean Macro dataset
na_columns <- colSums(is.na(data_macro)) >0 # there are no NA values in the dataset
#print(names(na_columns)[na_columns])
data_macro <- data_macro[, -which(names(data_macro) == 'date')] #only keep relevant columns for analysis
data_macro_filt <- data_macro %>%
  filter(Date >= as.Date('1980-11-01') & Date <= as.Date('2016-02-01')) 

### Merge datasets
df <- merge(data_commodity_filt, data_macro_filt, by='Date')
df <- merge(df, data_weather_filt, by='Date') # merged dataset for analysis
colnames(df) <- gsub("\\.", "_", colnames(df))
column_names <- names(df)
print(column_names) # 1 = Date, 2:55 = Commodity Prices, 56:62 = Macro inputs, 63:110 = Average temeratures by state

## Average the US Temperature
df$USA_Avg_Temp <- rowMeans(df[, 63:110], na.rm=TRUE) #average all states together
df <- df[, -c(63:110)]

## Correlation Analysis
#colnames(df)
dependent_vars <- df[, -c(56:63)] # exclude the predictors
predictor_vars <- df[, c(56:63)] # exclude the dependent variables 
dependent_vars <- dependent_vars[, !colnames(dependent_vars) %in% "Date"] #for correlation analysis exclude the date
dependent_vars_10 <- dependent_vars[, c(1:10)] # create several parts, as it is easier to look at
dependent_vars_20 <- dependent_vars[, c(11:20)]
dependent_vars_30 <- dependent_vars[, c(21:30)]
dependent_vars_40 <- dependent_vars[, c(31:40)]
dependent_vars_50 <- dependent_vars[, c(41:50)]
dependent_vars_60 <- dependent_vars[, c(51:54)]

cor_matrix_10 <- cor(dependent_vars_10, predictor_vars, use = "complete.obs")
corrplot(cor_matrix_10, method = "color", type = "upper", tl.cex = 0.7)

### LM test
predictors <- df[, (which(names(df)=='CPI')):ncol(df)] # define all macro and weather variables
model <- lm(Beef ~ ., data=cbind(df['Beef'], predictors)) # adjust the dependent variable for an overview of the significance
summary(model)
